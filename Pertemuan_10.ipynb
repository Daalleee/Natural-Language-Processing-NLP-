{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMhLhJFk8NPuMWxutkTptZP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daalleee/Natural-Language-Processing-NLP-/blob/main/Pertemuan_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_CC14HG2JtmO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# Ganti 'data.csv' dengan nama file Anda\n",
        "file_path='dataKlas.csv'\n",
        "# === LOAD & PERSIAPAN DATA ===\n",
        "# parsing manual\n",
        "data = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    next(f) # skip header\n",
        "    for line in f:\n",
        "        line = line.strip().strip('\"')\n",
        "        fields = line.split('\\t')\n",
        "        if len(fields) == 5:\n",
        "          data.append(fields)\n",
        "df = pd.DataFrame(data, columns=[\"text_id\", \"text_idn\",\n",
        "\"text_eng\", \"sentiment\", \"emotion\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bersihkan kolom 'emotion' dan 'sentiment'\n",
        "df['emotion'] = df['emotion'].str.strip().str.lower()\n",
        "df['sentiment'] = df['sentiment'].str.strip().str.lower()\n",
        "\n",
        "# Tampilkan label unik\n",
        "unique_sentiment = df['sentiment'].unique().tolist()\n",
        "unique_emotion = df['emotion'].unique().tolist()\n",
        "unique_sentiment, unique_emotion\n",
        "\n",
        "# === PECAH JADI KALIMAT ===\n",
        "import re\n",
        "\n",
        "def split_sentences(text):\n",
        "    # Pisah berdasarkan titik diikuti spasi atau akhir kalimat\n",
        "    return [s.strip() for s in re.split(r'[.!?]+(?=\\s|$)', text) if s.strip()]\n",
        "\n",
        "texts_train = []\n",
        "labels_train = []\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    # Lewati baris yang labelnya kosong\n",
        "    if pd.isna(row['text_idn']) or pd.isna(row['sentiment']) or pd.isna(row['emotion']):\n",
        "        continue\n",
        "\n",
        "    sentences = split_sentences(row['text_idn'])\n",
        "    for sentence in sentences:\n",
        "        texts_train.append(sentence)\n",
        "        label = row['sentiment'] + '+' + row['emotion']\n",
        "        labels_train.append(label)\n"
      ],
      "metadata": {
        "id": "1M8Cf7DZMxjo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "# Gabungkan teks dan label jadi satu list\n",
        "data_pairs = list(zip(texts_train, labels_train))\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(data_pairs)\n",
        "\n",
        "# Split 80% train, 20% test\n",
        "split_idx = int(0.8 * len(data_pairs))\n",
        "train_data = data_pairs[:split_idx]\n",
        "test_data = data_pairs[split_idx:]\n",
        "\n",
        "texts_train_split = [x[0] for x in train_data]\n",
        "labels_train_split = [x[1] for x in train_data]\n",
        "texts_test_split = [x[0] for x in test_data]\n",
        "labels_test_split = [x[1] for x in test_data]\n",
        "\n",
        "# === TRAINING ===\n",
        "classes = set(labels_train_split)\n",
        "class_word_counts = {c: {} for c in classes}\n",
        "total_words_in_class = {c: 0 for c in classes}\n",
        "doc_count_per_class = {c: 0 for c in classes}\n",
        "\n",
        "for text, label in zip(texts_train_split, labels_train_split):\n",
        "    doc_count_per_class[label] += 1\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    for w in words:\n",
        "        class_word_counts[label][w] = class_word_counts[label].get(w, 0) + 1\n",
        "        total_words_in_class[label] += 1\n",
        "\n",
        "vocab = set()\n",
        "for c in classes:\n",
        "    vocab.update(class_word_counts[c].keys())\n",
        "\n",
        "V = len(vocab)\n",
        "total_docs = len(texts_train_split)\n",
        "\n",
        "class_priors = {c: doc_count_per_class[c] / total_docs for c in classes}\n",
        "\n",
        "print(\"Jumlah kelas:\", classes)\n",
        "print(\"Total kata unik (ukuran vocab):\", V)\n",
        "for c in classes:\n",
        "    print(f\"Kelas {c}: {doc_count_per_class[c]} dokumen, {total_words_in_class[c]} total kata\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rR2yDr3Pv5O",
        "outputId": "9138d11f-b5dd-4b57-8777-5d6ea8f4e28c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah kelas: {'negative+sad', 'positive+happy', 'neutral+neutral', 'negative+anger', 'positive+love', 'negative+fear'}\n",
            "Total kata unik (ukuran vocab): 1578\n",
            "Kelas negative+sad: 93 dokumen, 1259 total kata\n",
            "Kelas positive+happy: 62 dokumen, 566 total kata\n",
            "Kelas neutral+neutral: 57 dokumen, 926 total kata\n",
            "Kelas negative+anger: 84 dokumen, 940 total kata\n",
            "Kelas positive+love: 50 dokumen, 383 total kata\n",
            "Kelas negative+fear: 65 dokumen, 744 total kata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import re\n",
        "\n",
        "def predict(text):\n",
        "    words = re.findall(r'\\w+', text.lower())\n",
        "    scores = {c: math.log(class_priors[c]) for c in classes}\n",
        "\n",
        "    for c in classes:\n",
        "        for w in words:\n",
        "            count = class_word_counts[c].get(w, 0)\n",
        "            prob_w_c = (count + 1) / (total_words_in_class[c] + V)\n",
        "            scores[c] += math.log(prob_w_c)\n",
        "\n",
        "    return max(scores, key=scores.get)\n",
        "\n",
        "# Contoh prediksi\n",
        "text = \"kelebihan kamar luas dan ada balkon di setiap kamar yang menghadap kolam renang\"\n",
        "print(predict(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj_T7B4yPwah",
        "outputId": "6bcfb8ff-b096-4394-b270-c8213231cbd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative+fear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi confusion matrix\n",
        "confusion = {true: {pred: 0 for pred in classes} for true in classes}\n",
        "\n",
        "correct = 0\n",
        "for text, true_label in zip(texts_test_split, labels_test_split):\n",
        "    pred_label = predict(text)\n",
        "    confusion[true_label][pred_label] += 1\n",
        "    if pred_label == true_label:\n",
        "        correct += 1\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = correct / len(texts_test_split)\n",
        "print(f\"\\nAkurasi pada data uji: {accuracy:.2f}\\n\")\n",
        "\n",
        "# Cetak confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "header = \"\\t\" + \"\\t\".join(classes)\n",
        "print(header)\n",
        "\n",
        "for true in classes:\n",
        "    row = [str(confusion[true][pred]) for pred in classes]\n",
        "    print(f\"{true}\\t\" + \"\\t\".join(row))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-fHYDj_QA1o",
        "outputId": "b72bf829-f20c-47ab-a574-11d767a57424"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Akurasi pada data uji: 0.32\n",
            "\n",
            "Confusion Matrix:\n",
            "\tnegative+sad\tpositive+happy\tneutral+neutral\tnegative+anger\tpositive+love\tnegative+fear\n",
            "negative+sad\t9\t2\t0\t2\t1\t2\n",
            "positive+happy\t4\t6\t1\t0\t3\t0\n",
            "neutral+neutral\t4\t1\t4\t8\t0\t0\n",
            "negative+anger\t11\t2\t2\t3\t0\t6\n",
            "positive+love\t0\t0\t2\t1\t6\t2\n",
            "negative+fear\t4\t5\t2\t2\t3\t5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents = texts_train\n",
        "\n",
        "# Bangun vocabulary dan representasi dokumen\n",
        "vocab = {}  # peta kata ke index\n",
        "word_counts = []  # list of dict untuk hitungan kata per dokumen\n",
        "\n",
        "for doc in documents:\n",
        "    words = re.findall(r'\\w+', doc.lower())\n",
        "    # Hitung frekuensi kata di dokumen ini\n",
        "    freq = {}\n",
        "    for w in words:\n",
        "        if w not in vocab:\n",
        "            vocab[w] = len(vocab)  # tambahkan kata baru ke vocab\n",
        "        idx = vocab[w]\n",
        "        # Update hitungan kata (bisa simpan by index atau by word, di sini by index lebih efisien)\n",
        "        freq[idx] = freq.get(idx, 0) + 1\n",
        "    word_counts.append(freq)\n",
        "\n",
        "D = len(documents)  # jumlah dokumen\n",
        "V = len(vocab)      # ukuran kosakata\n",
        "\n",
        "print(\"Total dokumen:\", D)\n",
        "print(\"Ukuran kosakata:\", V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS9lxLBdQPWC",
        "outputId": "232e4912-c067-40d6-f7a4-23cfc9f650ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total dokumen: 514\n",
            "Ukuran kosakata: 1863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Konversi dokumen-kata menjadi matriks dense numpy (D x V) untuk kemudahan NMF\n",
        "# Warning: ini bisa besar. Jika V sangat besar, pertimbangkan pakai subset vocab.\n",
        "D = len(documents)\n",
        "V = len(vocab)\n",
        "\n",
        "X = np.zeros((D, V))\n",
        "for d, freq in enumerate(word_counts):\n",
        "    for idx, count in freq.items():\n",
        "        X[d, idx] = count\n",
        "\n",
        "# Tentukan jumlah topik\n",
        "K = 5\n",
        "np.random.seed(0)\n",
        "W = np.random.rand(D, K)\n",
        "H = np.random.rand(K, V)\n",
        "\n",
        "# Fungsi untuk menghitung rekonstruksi (opsional, bisa untuk memonitor error)\n",
        "def reconstruct_error(X, W, H):\n",
        "    return np.linalg.norm(X - W.dot(H))\n",
        "\n",
        "# Iterasi update NMF\n",
        "for it in range(50):\n",
        "    # Update H\n",
        "    numerator = W.T.dot(X)  # shape K x V\n",
        "    denominator = W.T.dot(W).dot(H)  # shape K x V\n",
        "    # Tambahkan kecil epsilon pada denominator untuk hindari div zero\n",
        "    H *= numerator / (denominator + 1e-9)\n",
        "\n",
        "    # Update W\n",
        "    numerator = X.dot(H.T)  # shape D x K\n",
        "    denominator = W.dot(H).dot(H.T)  # shape D x K\n",
        "    W *= numerator / (denominator + 1e-9)\n",
        "\n",
        "    if it % 10 == 0:\n",
        "        err = reconstruct_error(X, W, H)\n",
        "        print(f\"Iterasi {it}, reconstruction error = {err:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAMNLEgqQi8j",
        "outputId": "bce1e789-0048-4854-a241-760698908cec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterasi 0, reconstruction error = 83.77\n",
            "Iterasi 10, reconstruction error = 79.17\n",
            "Iterasi 20, reconstruction error = 78.67\n",
            "Iterasi 30, reconstruction error = 78.58\n",
            "Iterasi 40, reconstruction error = 78.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ambil top 10 kata untuk setiap topik\n",
        "index_to_word = {idx: w for w, idx in vocab.items()}  # peta balik index ke kata\n",
        "\n",
        "top_words_per_topic = {}\n",
        "\n",
        "for k in range(K):\n",
        "    # Urutkan index kata berdasarkan bobot H[k] dari terbesar ke terkecil\n",
        "    top_indices = np.argsort(H[k, :])[::-1]  # descending sort\n",
        "    top_words = [index_to_word[idx] for idx in top_indices[:10]]\n",
        "    top_words_per_topic[k] = top_words\n",
        "    print(f\"Topik {k}: \" + \", \".join(top_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mrg4WupQvyg",
        "outputId": "e7486f8e-583b-4246-ce40-6884f8b1e765"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topik 0: saya, ini, kalo, game, bisa, gk, dulu, dari, malah, lagi\n",
            "Topik 1: nya, juga, ada, jalan, masuk, ga, malah, udah, mod, dan\n",
            "Topik 2: bisa, ke, dan, bug, sangat, map, gak, bagus, tidak, tapi\n",
            "Topik 3: di, saya, tapi, perbaiki, yg, bus, bug, itu, pas, ada\n",
            "Topik 4: yang, aku, ini, dan, jembatan, rawat, aja, w, buk, rasakan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Persiapkan struktur dokumen -> list of word indices per dokumen\n",
        "documents_word_indices = []\n",
        "for freq in word_counts:\n",
        "    doc_indices = []\n",
        "    for w_idx, count in freq.items():\n",
        "        doc_indices.extend([w_idx] * count)\n",
        "    documents_word_indices.append(doc_indices)\n",
        "\n",
        "# Parameter LDA\n",
        "K = 5\n",
        "alpha = 0.1\n",
        "beta = 0.01\n",
        "D = len(documents_word_indices)\n",
        "V = len(vocab)\n",
        "\n",
        "# Inisialisasi count\n",
        "doc_topic_counts = [[0] * K for _ in range(D)]\n",
        "topic_word_counts = [[0] * V for _ in range(K)]\n",
        "topic_counts = [0] * K\n",
        "assignments = [[] for _ in range(D)]\n",
        "\n",
        "# Random initialize topic for each token\n",
        "random.seed(42)\n",
        "for d, word_list in enumerate(documents_word_indices):\n",
        "    for w_idx in word_list:\n",
        "        k = random.randrange(K)\n",
        "        assignments[d].append(k)\n",
        "        doc_topic_counts[d][k] += 1\n",
        "        topic_word_counts[k][w_idx] += 1\n",
        "        topic_counts[k] += 1\n",
        "\n",
        "# Gibbs sampling\n",
        "iterations = 10\n",
        "for it in range(iterations):\n",
        "    for d, word_list in enumerate(documents_word_indices):\n",
        "        for i, w_idx in enumerate(word_list):\n",
        "            old_topic = assignments[d][i]\n",
        "\n",
        "            # Decrement counts for old_topic\n",
        "            doc_topic_counts[d][old_topic] -= 1\n",
        "            topic_word_counts[old_topic][w_idx] -= 1\n",
        "            topic_counts[old_topic] -= 1\n",
        "\n",
        "            # Hitung distribusi posterior\n",
        "            probabilities = []\n",
        "            for k in range(K):\n",
        "                p_doc = (doc_topic_counts[d][k] + alpha)\n",
        "                p_word = (topic_word_counts[k][w_idx] + beta) / (topic_counts[k] + V * beta)\n",
        "                probabilities.append(p_doc * p_word)\n",
        "\n",
        "            # Normalisasi\n",
        "            total_p = sum(probabilities)\n",
        "            probabilities = [p / total_p for p in probabilities]\n",
        "\n",
        "            # Sample new topic k berdasarkan distribusi probabilitas\n",
        "            r = random.random()\n",
        "            cumulative = 0.0\n",
        "            new_topic = K - 1\n",
        "            for k, p in enumerate(probabilities):\n",
        "                cumulative += p\n",
        "                if r <= cumulative:\n",
        "                    new_topic = k\n",
        "                    break\n",
        "\n",
        "            # Assign new_topic\n",
        "            assignments[d][i] = new_topic\n",
        "            doc_topic_counts[d][new_topic] += 1\n",
        "            topic_word_counts[new_topic][w_idx] += 1\n",
        "            topic_counts[new_topic] += 1\n",
        "\n",
        "    print(f\"Iterasi {it + 1} selesai\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju75y2TRQ3fK",
        "outputId": "ac4e579e-0442-41f0-cd98-0c22fb559d9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterasi 1 selesai\n",
            "Iterasi 2 selesai\n",
            "Iterasi 3 selesai\n",
            "Iterasi 4 selesai\n",
            "Iterasi 5 selesai\n",
            "Iterasi 6 selesai\n",
            "Iterasi 7 selesai\n",
            "Iterasi 8 selesai\n",
            "Iterasi 9 selesai\n",
            "Iterasi 10 selesai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak top 10 kata per topik dari hasil LDA\n",
        "index_to_word = {idx: w for w, idx in vocab.items()}\n",
        "\n",
        "for k in range(K):\n",
        "    # Ambil 10 kata dengan count tertinggi di topik k\n",
        "    top_indices = sorted(range(V), key=lambda v: topic_word_counts[k][v], reverse=True)\n",
        "    top_words = [index_to_word[idx] for idx in top_indices[:10]]\n",
        "    print(f\"Topik {k}: \" + \", \".join(top_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNBapm85RH3f",
        "outputId": "2ccd4168-4690-4ddd-98ef-30374f839d91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topik 0: yang, dan, ada, tidak, saya, lagi, jalan, lebih, game, ini\n",
            "Topik 1: di, aplikasi, ini, saya, juga, ada, tapi, ke, aja, sama\n",
            "Topik 2: gak, nya, malah, dan, update, yg, dari, mau, ya, sekarang\n",
            "Topik 3: di, nya, bisa, game, saya, tolong, bug, perbaiki, ada, bagus\n",
            "Topik 4: aku, nya, ini, suka, ga, bagus, sendiri, dan, untuk, sangat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI728UpsRN0d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}