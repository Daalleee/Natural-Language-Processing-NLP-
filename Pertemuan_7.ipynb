{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPDMyByDP4DzUgpAYokI3HH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daalleee/Natural-Language-Processing-NLP-/blob/main/Pertemuan_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes 1**"
      ],
      "metadata": {
        "id": "YOsCycLpIHGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE1UO5vk_d1A",
        "outputId": "1cf9c8c8-bb4f-4a20-e958-ba1e6c173aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam Probability: 2.0929167208772063e-07\n",
            "Not Spam Probability: 3.9245856642232505e-09\n",
            "Prediction: Spam\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Data Latihan\n",
        "spam_docs = [\"Promo besar hari ini diskon 50%!\", \"Gratis hadiah untuk pelanggan setia!\", \"Segera klaim hadiah spesial kamu!\"]\n",
        "not_spam_docs = [\"Halo, bagaimana kabarmu hari ini?\", \"Jangan lupa meeting besok pukul 10.00\", \"Dokumen penting sudah dikirim ke email\"]\n",
        "\n",
        "# Tokenisasi\n",
        "def tokenize(text):\n",
        "    return text.lower().replace(\"!\", \"\").replace(\"%\", \"\").split()\n",
        "\n",
        "spam_words = [word for doc in spam_docs for word in tokenize(doc)]\n",
        "not_spam_words = [word for doc in not_spam_docs for word in tokenize(doc)]\n",
        "\n",
        "# Hitung Probabilitas\n",
        "spam_counts = Counter(spam_words)\n",
        "not_spam_counts = Counter(not_spam_words)\n",
        "V = len(set(spam_words + not_spam_words))\n",
        "total_spam = len(spam_words)\n",
        "total_not_spam = len(not_spam_words)\n",
        "\n",
        "def get_prob(word, category_counts, total_count):\n",
        "    return (category_counts.get(word, 0) + 1) / (total_count + V)\n",
        "\n",
        "# Prediksi Email Baru\n",
        "email = \"Hadiah besar gratis untuk kamu!\"\n",
        "words = tokenize(email)\n",
        "spam_prob = math.prod([get_prob(word, spam_counts, total_spam) for word in words])\n",
        "not_spam_prob = math.prod([get_prob(word, not_spam_counts, total_not_spam) for word in words])\n",
        "print(\"Spam Probability:\", spam_prob)\n",
        "print(\"Not Spam Probability:\", not_spam_prob)\n",
        "print(\"Prediction:\", \"Spam\" if spam_prob > not_spam_prob else \"Not Spam\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Naive Bayes 2**"
      ],
      "metadata": {
        "id": "VEWMvZssIQNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "    \"Promo besar hari ini diskon 50%!\", \"Gratis hadiah untuk pelanggan setia!\", \"Segera klaim hadiah spesial kamu!\", # spam\n",
        "    \"Halo, bagaimana kabarmu hari ini?\", \"Jangan lupa meeting besok pukul 10.00\", \"Dokumen penting sudah dikirim ke email\" # Not spam\n",
        "]\n",
        "labels = [\"spam\", \"spam\", \"spam\", \"not spam\", \"not spam\", \"not spam\"]\n",
        "\n",
        "# 1. Tokenisasi dan Stopword Removal\n",
        "stopwords = [\"ke\"]\n",
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    return [token for token in tokens if token not in stopwords]\n",
        "processed_texts = [preprocess(text) for text in texts]\n",
        "\n",
        "# 2. Buat Vocabulary\n",
        "vocab = {}\n",
        "index = 0\n",
        "for text in processed_texts:\n",
        "    for token in text:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "\n",
        "# 3. Konversi ke Vektor (Bag of Words)\n",
        "def text_to_vector(tokens, vocab):\n",
        "    vector = [0] * len(vocab)\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            vector[vocab[token]] += 1\n",
        "    return vector\n",
        "\n",
        "X = [text_to_vector(tokens, vocab) for tokens in processed_texts]\n",
        "y = [1 if label == \"not spam\" else 0 for label in labels]\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, alpha=1):\n",
        "        self.alpha = alpha # Laplace smoothing\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = len(X), len(X[0])\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "        self.priors = np.zeros(n_classes)\n",
        "        for c in self.classes:\n",
        "            self.priors[c] = (sum(y == c) ) / (n_samples )\n",
        "            #self.priors[c] = (sum(y == c) + self.alpha) / (n_samples + self.alpha * n_classes)\n",
        "        self.likelihoods = np.zeros((n_classes, n_features))\n",
        "        for c in self.classes:\n",
        "            X_c = [X[i] for i in range(n_samples) if y[i] == c]\n",
        "            total_words_c = sum(sum(x) for x in X_c)\n",
        "            for j in range(n_features):\n",
        "                count_j = sum(x[j] for x in X_c)\n",
        "                self.likelihoods[c][j] = (count_j + self.alpha) / (total_words_c + self.alpha * n_features)\n",
        "        print(self.likelihoods)\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            posteriors = []\n",
        "            for c in self.classes:\n",
        "                prior = self.priors[c]\n",
        "                likelihood = math.prod(self.likelihoods[c][j] for j, val in enumerate(x) if val > 0)\n",
        "                posteriors.append(prior * likelihood)\n",
        "            predictions.append(self.classes[np.argmax(posteriors)])\n",
        "            print(posteriors)\n",
        "        return predictions\n",
        "\n",
        "# Contoh Penggunaan\n",
        "nb = NaiveBayes(alpha=1)\n",
        "nb.fit(X, y)\n",
        "test_text = preprocess(\"Hadiah besar gratis untuk kamu!\")\n",
        "test_vector = text_to_vector(test_text, vocab)\n",
        "print(nb.predict([test_vector]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt22O_VuFPbt",
        "outputId": "17046b96-86cd-4898-89e8-cac217a9753e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826\n",
            "  0.04347826 0.06521739 0.04347826 0.04347826 0.04347826 0.04347826\n",
            "  0.04347826 0.04347826 0.04347826 0.02173913 0.02173913 0.02173913\n",
            "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913\n",
            "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913]\n",
            " [0.02173913 0.02173913 0.04347826 0.02173913 0.02173913 0.02173913\n",
            "  0.02173913 0.02173913 0.02173913 0.02173913 0.02173913 0.02173913\n",
            "  0.02173913 0.02173913 0.02173913 0.04347826 0.04347826 0.04347826\n",
            "  0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826\n",
            "  0.04347826 0.04347826 0.04347826 0.04347826 0.04347826 0.04347826]]\n",
            "[np.float64(1.1652579733553663e-07), np.float64(2.4276207778236796e-09)]\n",
            "[np.int64(0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SVM**"
      ],
      "metadata": {
        "id": "4r5MbU5uKMio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "texts_train = [\n",
        "    \"Promo besar hari ini diskon 50%!\", \"Gratis hadiah untuk pelanggan setia!\", \"Segera klaim hadiah spesial kamu!\",\n",
        "    \"Halo, bagaimana kabarmu hari ini?\", \"Jangan lupa meeting besok pukul 10.00\", \"Dokumen penting sudah dikirim ke email\"\n",
        "]\n",
        "labels_train = [\"spam\", \"spam\", \"spam\", \"not spam\", \"not spam\", \"not spam\"]\n",
        "texts_test = [\n",
        "    \"Kamu mendapatkan hadiah besar!\", \"Besok ada ujian penting!\"\n",
        "]\n",
        "stopwords = [\"ke\", \"di\", \"dari\", \"untuk\", \"pada\"]\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    return [token for token in tokens if token not in stopwords]\n",
        "\n",
        "processed_train = [preprocess(text) for text in texts_train]\n",
        "processed_test = [preprocess(text) for text in texts_test]\n",
        "\n",
        "vocab_train = {}\n",
        "index = 0\n",
        "for text in processed_train:\n",
        "    for token in text:\n",
        "        if token not in vocab_train:\n",
        "            vocab_train[token] = index\n",
        "            index += 1\n",
        "print(f\"Vocabulary dari training: {vocab_train}\")\n",
        "\n",
        "def text_to_vector(tokens, vocab):\n",
        "    vector = [0] * len(vocab) # Gunakan panjang vocab dari training\n",
        "    for token in tokens:\n",
        "        if token in vocab:\n",
        "            vector[vocab[token]] += 1\n",
        "    return vector\n",
        "\n",
        "X_train = np.array([text_to_vector(tokens, vocab_train) for tokens in processed_train])\n",
        "X_test = np.array([text_to_vector(tokens, vocab_train) for tokens in processed_test]) # Gunakan vocab_train\n",
        "y = np.array([1 if label == \"not spam\" else -1 for label in labels_train])\n",
        "\n",
        "print(\"\\n X_train (BoW dari training data):\")\n",
        "print(X_train)\n",
        "print(\"\\n X_test (BoW dari testing data):\")\n",
        "print(X_test)\n",
        "\n",
        "# Inisialisasi bobot dan bias\n",
        "w = np.zeros(X_train.shape[1]) # Panjang bobot = jumlah fitur\n",
        "b = 0\n",
        "eta = 1 # Learning rate\n",
        "epochs = 5 # Jumlah iterasi pembelajaran\n",
        "\n",
        "# Training SVM menggunakan SGD dengan Subgradien Hinge Loss\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\n Epoch {epoch + 1}\")\n",
        "    for i in range(len(X_train)):\n",
        "        # Mengecek kondisi Hinge Loss: y * (w·x + b) < 1\n",
        "        if y[i] * (np.dot(w, X_train[i]) + b) < 1:\n",
        "            # Update bobot jika salah klasifikasi atau berada dalam margin\n",
        "            w = w + eta * y[i] * X_train[i]\n",
        "            b = b + eta * y[i]\n",
        "            print(f\" ➜ Update: w = {w}, b = {b}\")\n",
        "        else:\n",
        "            print(f\" ➜ Tidak ada update untuk sampel ke-{i+1}\")\n",
        "\n",
        "# Hasil akhir\n",
        "print(\"\\nModel SVM selesai dilatih!\")\n",
        "print(f\"Bobot akhir: {w}\")\n",
        "print(f\"Bias akhir: {b}\")\n",
        "\n",
        "# Contoh Prediksi\n",
        "def predict(X_test):\n",
        "    return np.sign(np.dot(X_test, w) + b)\n",
        "\n",
        "predictions = predict(X_test)\n",
        "print(\"\\nPrediksi untuk sampel uji:\")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "    label = \"Spam\" if predictions[i] == 1 else \"Not Spam\"\n",
        "    print(f\" - Sampel {i+1}: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoZCyd5IG5SO",
        "outputId": "5c328b3f-2d1a-4e7a-b79a-ed7a55831a9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary dari training: {'promo': 0, 'besar': 1, 'hari': 2, 'ini': 3, 'diskon': 4, '50%!': 5, 'gratis': 6, 'hadiah': 7, 'pelanggan': 8, 'setia!': 9, 'segera': 10, 'klaim': 11, 'spesial': 12, 'kamu!': 13, 'halo,': 14, 'bagaimana': 15, 'kabarmu': 16, 'ini?': 17, 'jangan': 18, 'lupa': 19, 'meeting': 20, 'besok': 21, 'pukul': 22, '10.00': 23, 'dokumen': 24, 'penting': 25, 'sudah': 26, 'dikirim': 27, 'email': 28}\n",
            "\n",
            " X_train (BoW dari training data):\n",
            "[[1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1]]\n",
            "\n",
            " X_test (BoW dari testing data):\n",
            "[[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
            "\n",
            " Epoch 1\n",
            " ➜ Update: w = [-1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], b = -1\n",
            " ➜ Tidak ada update untuk sampel ke-2\n",
            " ➜ Tidak ada update untuk sampel ke-3\n",
            " ➜ Update: w = [-1. -1.  0. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], b = 0\n",
            " ➜ Update: w = [-1. -1.  0. -1. -1. -1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.], b = 1\n",
            " ➜ Tidak ada update untuk sampel ke-6\n",
            "\n",
            " Epoch 2\n",
            " ➜ Tidak ada update untuk sampel ke-1\n",
            " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.], b = 0\n",
            " ➜ Tidak ada update untuk sampel ke-3\n",
            " ➜ Tidak ada update untuk sampel ke-4\n",
            " ➜ Tidak ada update untuk sampel ke-5\n",
            " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.], b = 1\n",
            "\n",
            " Epoch 3\n",
            " ➜ Tidak ada update untuk sampel ke-1\n",
            " ➜ Tidak ada update untuk sampel ke-2\n",
            " ➜ Update: w = [-1. -1.  0. -1. -1. -1. -1. -2. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.], b = 0\n",
            " ➜ Tidak ada update untuk sampel ke-4\n",
            " ➜ Tidak ada update untuk sampel ke-5\n",
            " ➜ Tidak ada update untuk sampel ke-6\n",
            "\n",
            " Epoch 4\n",
            " ➜ Tidak ada update untuk sampel ke-1\n",
            " ➜ Tidak ada update untuk sampel ke-2\n",
            " ➜ Tidak ada update untuk sampel ke-3\n",
            " ➜ Tidak ada update untuk sampel ke-4\n",
            " ➜ Tidak ada update untuk sampel ke-5\n",
            " ➜ Tidak ada update untuk sampel ke-6\n",
            "\n",
            " Epoch 5\n",
            " ➜ Tidak ada update untuk sampel ke-1\n",
            " ➜ Tidak ada update untuk sampel ke-2\n",
            " ➜ Tidak ada update untuk sampel ke-3\n",
            " ➜ Tidak ada update untuk sampel ke-4\n",
            " ➜ Tidak ada update untuk sampel ke-5\n",
            " ➜ Tidak ada update untuk sampel ke-6\n",
            "\n",
            "Model SVM selesai dilatih!\n",
            "Bobot akhir: [-1. -1.  0. -1. -1. -1. -1. -2. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.\n",
            "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
            "Bias akhir: 0\n",
            "\n",
            "Prediksi untuk sampel uji:\n",
            "[-1.  1.]\n",
            " - Sampel 1: Not Spam\n",
            " - Sampel 2: Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Random Forest**"
      ],
      "metadata": {
        "id": "146l7jogKU_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Dataset\n",
        "data = [\n",
        "    {\"hadiah\": 1, \"besar\": 1, \"gratis\": 1, \"untukmu\": 1, \"hari\": 1, \"ini\": 1, \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \"selamat\": 0, \"anda\": 0, \"memenangkan\": 0, \"jangan\": 0, \"lupa\": 0, \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0, \"label\": 1},\n",
        "    {\"hadiah\": 0, \"besar\": 0, \"gratis\": 0, \"untukmu\": 0, \"hari\": 0, \"ini\": 0, \"besok\": 1, \"kita\": 1, \"ada\": 1, \"pertemuan\": 1, \"kampus\": 1, \"selamat\": 0, \"anda\": 0, \"memenangkan\": 0, \"jangan\": 0, \"lupa\": 0, \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0, \"label\": -1},\n",
        "    {\"hadiah\": 1, \"besar\": 1, \"gratis\": 0, \"untukmu\": 0, \"hari\": 0, \"ini\": 0, \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \"selamat\": 1, \"anda\": 1, \"memenangkan\": 1, \"jangan\": 0, \"lupa\": 0, \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0, \"label\": 1},\n",
        "    {\"hadiah\": 0, \"besar\": 0, \"gratis\": 0, \"untukmu\": 0, \"hari\": 0, \"ini\": 0, \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \"selamat\": 0, \"anda\": 0, \"memenangkan\": 0, \"jangan\": 1, \"lupa\": 1, \"tugas\": 1, \"harus\": 1, \"dikumpulkan\": 1, \"terpilih\": 0, \"spesial\": 0, \"label\": -1},\n",
        "    {\"hadiah\": 1, \"besar\": 0, \"gratis\": 0, \"untukmu\": 0, \"hari\": 0, \"ini\": 0, \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \"selamat\": 0, \"anda\": 1, \"memenangkan\": 0, \"jangan\": 0, \"lupa\": 0, \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 1, \"spesial\": 1, \"label\": 1}\n",
        "]\n",
        "\n",
        "# Fungsi untuk menghitung Gini Impurity\n",
        "def gini_impurity(data):\n",
        "    total = len(data)\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    spam_count = sum(1 for row in data if row[\"label\"] == 1)\n",
        "    not_spam_count = total - spam_count\n",
        "    p_spam = spam_count / total\n",
        "    p_not_spam = not_spam_count / total\n",
        "    return 1 - (p_spam ** 2 + p_not_spam ** 2)\n",
        "\n",
        "# Fungsi untuk membagi dataset berdasarkan fitur tertentu\n",
        "def split_data(data, feature):\n",
        "    left = [row for row in data if row[feature] == 0]\n",
        "    right = [row for row in data if row[feature] == 1]\n",
        "    return left, right\n",
        "\n",
        "# Fungsi untuk menemukan fitur terbaik untuk split\n",
        "def best_split(data):\n",
        "    best_feature = None\n",
        "    best_gini = 1\n",
        "    best_left, best_right = None, None\n",
        "    for feature in data[0].keys():\n",
        "        if feature == \"label\":\n",
        "            continue\n",
        "        left, right = split_data(data, feature)\n",
        "        gini_left = gini_impurity(left)\n",
        "        gini_right = gini_impurity(right)\n",
        "        gini_split = (len(left) / len(data)) * gini_left + (len(right) / len(data)) * gini_right\n",
        "        if gini_split < best_gini:\n",
        "            best_gini = gini_split\n",
        "            best_feature = feature\n",
        "            best_left, best_right = left, right\n",
        "    return best_feature, best_left, best_right\n",
        "\n",
        "# Kelas Decision Tree\n",
        "class DecisionTree:\n",
        "    def __init__(self, depth=2):\n",
        "        self.depth = depth\n",
        "        self.tree = None\n",
        "\n",
        "    def build_tree(self, data, depth=0):\n",
        "        if len(set(row[\"label\"] for row in data)) == 1 or depth >= self.depth:\n",
        "            return {\"prediction\": max(set(row[\"label\"] for row in data), key=[row[\"label\"] for row in data].count)}\n",
        "        feature, left, right = best_split(data)\n",
        "        if not left or not right:\n",
        "            return {\"prediction\": max(set(row[\"label\"] for row in data), key=[row[\"label\"] for row in data].count)}\n",
        "        return {\n",
        "            \"feature\": feature,\n",
        "            \"left\": self.build_tree(left, depth + 1),\n",
        "            \"right\": self.build_tree(right, depth + 1)\n",
        "        }\n",
        "\n",
        "    def fit(self, data):\n",
        "        self.tree = self.build_tree(data)\n",
        "\n",
        "    def predict_one(self, row, node):\n",
        "        if \"prediction\" in node:\n",
        "            return node[\"prediction\"]\n",
        "        if row[node[\"feature\"]] == 0:\n",
        "            return self.predict_one(row, node[\"left\"])\n",
        "        else:\n",
        "            return self.predict_one(row, node[\"right\"])\n",
        "\n",
        "    def predict(self, data):\n",
        "        return [self.predict_one(row, self.tree) for row in data]\n",
        "\n",
        "class RandomForest:\n",
        "    def __init__(self, n_trees=3, sample_size=0.8, depth=2):\n",
        "        self.n_trees = n_trees\n",
        "        self.sample_size = sample_size\n",
        "        self.depth = depth\n",
        "        self.trees = []\n",
        "\n",
        "    def bootstrap_sample(self, data):\n",
        "        return random.sample(data, int(len(data) * self.sample_size))\n",
        "\n",
        "    def fit(self, data):\n",
        "        for _ in range(self.n_trees):\n",
        "            sample = self.bootstrap_sample(data)\n",
        "            tree = DecisionTree(depth=self.depth)\n",
        "            tree.fit(sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict_one(self, row):\n",
        "        predictions = [tree.predict_one(row, tree.tree) for tree in self.trees]\n",
        "        return max(set(predictions), key=predictions.count)\n",
        "\n",
        "    def predict(self, data):\n",
        "        return [self.predict_one(row) for row in data]\n",
        "\n",
        "rf = RandomForest(n_trees=3, depth=2)\n",
        "rf.fit(data)\n",
        "\n",
        "# Data uji\n",
        "test_sms = {\"hadiah\": 1, \"besar\": 0, \"gratis\": 1, \"untukmu\": 0, \"hari\": 0, \"ini\": 0, \"besok\": 0, \"kita\": 0, \"ada\": 0, \"pertemuan\": 0, \"kampus\": 0, \"selamat\": 0, \"anda\": 1, \"memenangkan\": 1, \"jangan\": 0, \"lupa\": 0, \"tugas\": 0, \"harus\": 0, \"dikumpulkan\": 0, \"terpilih\": 0, \"spesial\": 0}\n",
        "\n",
        "# Prediksi menggunakan Random Forest\n",
        "prediction = rf.predict_one(test_sms)\n",
        "print(\"Prediksi:\", \"Spam\" if prediction == 1 else \"Not Spam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLGGhyDEG8WB",
        "outputId": "9499e958-34d5-49e9-b039-f777dd30ad98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediksi: Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Logistic Regression**"
      ],
      "metadata": {
        "id": "27Ze1VS9KeeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Dataset (fitur dan label)\n",
        "X = [\n",
        "    [1, 0], # Hadiah besar menanti kamu! → Spam\n",
        "    [0, 1], # Tugas harus dikumpulkan! → Not Spam\n",
        "    [1, 0], # Gratis hadiah untukmu! → Spam\n",
        "    [0, 1], # Jangan lupa tugas kuliah! → Not Spam\n",
        "    [1, 0], # Selamat! Kamu mendapat hadiah! → Spam\n",
        "]\n",
        "y = [1, 0, 1, 0, 1] # 1 = Spam, 0 = Not Spam\n",
        "\n",
        "# Fungsi sigmoid\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + math.exp(-z))\n",
        "\n",
        "# Fungsi prediksi\n",
        "def predict(X, w, b):\n",
        "    preds = []\n",
        "    for x in X:\n",
        "        z = sum(w[i] * x[i] for i in range(len(x))) + b\n",
        "        preds.append(sigmoid(z))\n",
        "    return preds\n",
        "\n",
        "# Fungsi pembaruan bobot dengan Gradient Descent\n",
        "def train_logistic_regression(X, y, lr=0.1, epochs=100):\n",
        "    w = [0.0 for _ in range(len(X[0]))] # inisialisasi bobot\n",
        "    b = 0.0 # bias\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(X)):\n",
        "            z = sum(w[j] * X[i][j] for j in range(len(X[0]))) + b\n",
        "            pred = sigmoid(z)\n",
        "            error = y[i] - pred\n",
        "            # update bobot dan bias\n",
        "            for j in range(len(X[0])):\n",
        "                w[j] += lr * error * X[i][j]\n",
        "            b += lr * error\n",
        "    return w, b\n",
        "\n",
        "# Training\n",
        "weights, bias = train_logistic_regression(X, y, lr=0.1, epochs=100)\n",
        "\n",
        "# Prediksi contoh baru\n",
        "test_input = [0, 1] # Teks: \"Tugas penting menanti kamu\"\n",
        "z = sum(weights[i] * test_input[i] for i in range(len(test_input))) + bias\n",
        "prob = sigmoid(z)\n",
        "print(\"Probabilitas Spam:\", round(prob, 4))\n",
        "print(\"Prediksi:\", \"Spam\" if prob >= 0.5 else \"Not Spam\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qILt0QMbG_Ai",
        "outputId": "c7c1601f-3dda-4bae-9a99-3737e37f1d7a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilitas Spam: 0.0521\n",
            "Prediksi: Not Spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BERT dengan Logistic Regression**"
      ],
      "metadata": {
        "id": "NcqQ5yk5KpnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import math\n",
        "\n",
        "# 1. Load IndoBERT (tanpa classifier head)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "# 2. Dataset: teks dan label\n",
        "texts = [\n",
        "    \"Hadiah besar menanti kamu!\", # Spam\n",
        "    \"Tugas harus dikumpulkan!\", # Not Spam\n",
        "    \"Gratis hadiah untukmu!\", # Spam\n",
        "    \"Jangan lupa tugas kuliah!\", # Not Spam\n",
        "    \"Selamat! Kamu mendapat hadiah!\" # Spam\n",
        "]\n",
        "labels = [1, 0, 1, 0, 1]\n",
        "\n",
        "# 3. Fungsi: Ambil embedding [CLS] dari BERT\n",
        "def get_cls_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy() # shape: (768,)\n",
        "    return cls_embedding\n",
        "\n",
        "# 4. Ambil semua embedding\n",
        "X = [get_cls_embedding(text) for text in texts]\n",
        "y = labels\n",
        "\n",
        "# 5. Fungsi Sigmoid\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + math.exp(-z))\n",
        "\n",
        "# 6. Logistic Regression Manual\n",
        "def train_logistic_regression(X, y, lr=0.01, epochs=10):\n",
        "    w = [0.0] * len(X[0]) # 768 dimensi\n",
        "    b = 0.0\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(len(X)):\n",
        "            z = sum(w[j] * X[i][j] for j in range(len(w))) + b\n",
        "            pred = sigmoid(z)\n",
        "            error = y[i] - pred\n",
        "            for j in range(len(w)):\n",
        "                w[j] += lr * error * X[i][j]\n",
        "            b += lr * error\n",
        "    return w, b\n",
        "\n",
        "# 7. Latih model\n",
        "weights, bias = train_logistic_regression(X, y)\n",
        "\n",
        "# 8. Prediksi input baru\n",
        "def predict(text):\n",
        "    x = get_cls_embedding(text)\n",
        "    z = sum(weights[i] * x[i] for i in range(len(x))) + bias\n",
        "    prob = sigmoid(z)\n",
        "    return prob, \"Spam\" if prob >= 0.5 else \"Not Spam\"\n",
        "\n",
        "# 9. Uji prediksi\n",
        "test_text = \"Tugas penting menanti kamu!\"\n",
        "prob, label = predict(test_text)\n",
        "print(f\"Probabilitas Spam: {prob:.4f} → Prediksi: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L7ohRlCHBP1",
        "outputId": "59596d83-a0be-4786-950b-613c3b5a5d1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilitas Spam: 0.4866 → Prediksi: Not Spam\n"
          ]
        }
      ]
    }
  ]
}